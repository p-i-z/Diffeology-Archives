%%====================================================================% MARK: - Appendix A: Lagrange, Poisson, quelle histoire!%%====================================================================\chapter{Lagrange, Poisson, quelle histoire!}\label{AnnLP}Dans les cours de mécanique actuels,il est plus fréquent de voir cités les {\em crochets de Poisson} plutôt que les {\em crochets} ou les {\em parenthèses de Lagrange};or,il ne fait aucun doute que l'introduction des divers types de crochets/parenthèses en mécanique analytique est due à J.-L. Lagrange.Quel a donc été le rôle de Poisson dans cette affaire?On peut lire dans le premier article de Lagrange sur ce sujet \cite[pp.~717--718]{Lagrange2},le texte suivant:\begin{quote}  «Dans un mémoire lu à l'académie de Berlin en 1776,  je considérai d'une manière directe les variations auxquelles peut être sujet le grand axe d'une planète par les forces perturbatrices provenant de l'action des autres planètes,  et je réduisis ces variations à une formule générale et très simple qui,  ne dépendant que de la différentielle partielle d'une fonction finie relativement au mouvement moyen de la planète,  fait voir tout de suite que le grand axe ne peut jamais contenir aucun terme proportionnel au temps,  quelque loin qu'on continue l'approximation par rapport aux excentricités et aux inclinaisons des orbites,  mais en s'arrêtant à la première approximation par rapport aux termes proportionnels aux masses des planètes.  On n'avait pas été plus loin sur ce point;  mais M. Poisson y a fait un pas de plus dans le {\em Mémoire} qu'il a lu il y a deux mois à la classe,  {\em sur les inégalités séculaires des moyens mouvements des planètes},  et dont nous avons fait le Rapport dans la dernière séance.  Il a poussé l'approximation de la même formule jusqu'aux termes affectés des carrés et des produits des masses,  en ayant égard dans cette formule à la variation des éléments que j'avais regardés comme constants dans la première approximation.  \noindent [\ldots]  Cette découverte de M. Poisson a réveillé mon attention sur un objet qui m'avait autrefois beaucoup occupé,  et que j'avais ensuite totalement perdu de vue.  Il me parut que le résultat qu'il venait de trouver par le moyen des formules qui représentent le mouvement elliptique était un résultat analytique dépendant de la forme des équations différentielles et des conditions de la variabilité des constantes,  et que l'on devait y arriver par la seule force de l'analyse,  sans connaître les expressions particulières des quantités relatives à l'orbite elliptique.  \noindent [\ldots]  J'ai obtenu des formules qui donnent les différentielles de ces variations sous une forme plus simple que celle des formules connues jusqu'à présent.  \noindent [\ldots]  Ces formules ont de plus l'avantage que,  étant appliquées aux variations du grand axe,  on en voit naître tout de suite des expressions analogues à celles auxquelles M. Poisson n'est parvenu que par des réductions heureuses des formules déduites de la considération du mouvement elliptique.»\end{quote}Ces formules,auxquelles Lagrange fait allusion,sont celles qui ont été exposées dans notre première partie,chapitre \ref{parvarconst},et qui expriment les forces de perturbation relativement aux éléments des planètes en fonction de la variation des constantes et des parenthèses de Lagrange (éq. \ref{eq_D}).Quant à l'article de Poisson auquel Lagrange fait référence,il est publié dans le cahier \no 15 du {\em Journal de l'école polytechnique} \cite{Poisson1}.Comme on peut le constater à sa lecture,cet article de Poisson ne comporte aucune définition des {\em crochets de Poisson}.Comme le dit Lagrange,la méthode est traditionnelle,les calculs des perturbations y sont simplement poussés davantage que dans les articles précédents de Laplace ou de Lagrange.Alors comment s'explique cette apparition des crochets de Poisson ?Depuis longtemps,Lagrange avait l'habitude d'utiliser des {\em parenthèses} ou {\em crochets} dans les développements en séries perturbatives,où ils apparaissent comme les coefficients des différentielles des éléments perturbés,Lagrange y regroupait l'ensemble des variables dont ces coefficients dépendaient.Ces notations sont utilisées,en particulier,dans son article de 1781 sur la {\em Théorie des variations séculaires} \cite{Lagrange8}.Il n'est donc pas étonnant de les voir apparaître à nouveau dans ce {\em Mémoire sur la théorie des variations des éléments des planètes} en 1808.Seulement,comme nous l'avons vu dans les chapitres précédents,dans le cas particulier de la variation des constantes dans les problèmes de la mécanique,cette notation,qui n'était qu'une commodité jusqu'alors,révèle une structure sous-jacente tout à fait particulière,connue aujourd'hui comme {\em structure symplectique} de l'espace des solutions du système des planètes.Plus précisément:ces parenthèses sont les coordonnées covariantes de cette structure symplectique.Quelques mois après son premier mémoire,Lagrange comprend la portée universelle de sa découverte et généralise sa méthode à tous les problèmes de la mécanique.Pour apprécier ensuite l'apport de Poisson,il est nécessaire d'entrer à nouveau au c\oe ur de cette méthode:Lagrange développe les forces perturbatrices par rapport aux éléments kepleriens des planètes (et de façon plus générale par rapport aux constantes d'intégration du système).Ces forces sont des combinaisons linéaires des variations des éléments eux-mêmes et les coefficients de ces variations sont les parenthèses de Lagrange.Pour obtenir la variation des éléments des planètes,en fonctions des forces perturbatrices,il faut alors inverser le système linéaire ainsi obtenu dont les coefficients sont les parenthèses.Lagrange dit lui-même \cite[p. 731]{Lagrange2}:\begin{quote}  « Il s'ensuit qu'on pourra également exprimer les différentielles ${da \over dt}$, ${db \over dt}$, ${dc \over dt}$,\ldots par les différences partielles de la fonction $\Omega$ relatives aux éléments $a$, $b$, $c$,\ldots,  multipliées par de simples fonctions de ces quantités sans $t$;  car il n'y aura qu'à déduire les valeurs de ces différentielles des six équations précédentes par les méthodes ordinaires de l'élimination,  et il est visible qu'elles seront toutes de la forme  $$    \left( A{d\Omega\over da} + B{d\Omega\over db} + C{d\Omega\over dc} + F{d\Omega\over df} + G{d\Omega\over dg} + H{d\Omega\over dh}\right)dt,    $$  dans laquelle les coefficients $A$, $B$, $C$, $F$,\ldots ne seront donnés que par les coefficients $(a,b)$, $(a,c)$, $(b,c)$, \ldots,  et ne seront par conséquent que de simples fonctions de ces éléments sans $t$;  ce qui fournit un Théorème très-important et très-utile dans la théorie des perturbations des planètes.»\end{quote}Ce sont évidemment ces coefficients $A$, $B$, $C$, $F$, \ldots\ qui,considérés par Poisson comme fonctions des conditions initiales du mouvement\footnote{Remarquez la différence de point de vue:pour Lagrange,un ensemble de constantes d'intégration $(a,b,c,\ldots)$ définit un mouvement:ce sont les formules (\ref{eq_xy1}) et (\ref{eq_xy2});les positions et vitesses sont alors fonctions des constantes d'intégration.Poisson considère les constantes d'intégration comme fonctions des conditions initiales.C'est cette différence de point de vue qui conduit naturellement à des formules inverses les unes des autres.} (temps, position, vitesse) vont devenir les {\em crochets de Poisson} des constantes $a$, $b$, $c$, \ldots du mouvement.Poisson donne leur construction dans son mémoire intitulé {\em Sur la variation des constantes arbitraires dans les questions de Mécanique},publié à la suite du mémoire {\em Sur la théorie générale de la variation des constantes arbitraires dans tous les problèmes de la mécanique} de Lagrange \cite{Lagrange3}.Voici ce que Poisson écrit dans ce mémoire,lu à l'Institut le 16 octobre 1809 et publié dans le {\em Journal de l'école polytechnique} \cite[pp. 268--269]{Poisson2}:\begin{quote}  « Les formules du dernier Mémoire de M. Lagrange sont inverses des nôtres :  elles donnent les différences partielles de la même fonction,  au moyen des différentielles des constantes arbitraires.  Je les rapporte dans la suite de ce mémoire afin de les comparer à celles que nous avons obtenues,  et de montrer la singulière analogie qui existe entre ces deux genres de formule.»\end{quote}\noindent Il ajoute plus loin,aux page 289--290 du même mémoire,faisant référence au Théorème de Lagrange cité plus haut :\begin{quote}  «M. Lagrange démontre directement que les quantités $[a,b]$, $[a,c]$, $[a,b]$, \&c. sont des fonctions de $a$, $b$, $c$, $e$, $f$, $g$, $h$,  qui ne renferment pas le temps d'une manière explicite;  d'où l'on conclut ensuite que,  par des éliminations entre les équations que nous citons,  on obtiendra des valeurs de $da$, $db$, $dc$, $de$, $df$, $dg$, $dh$,  exprimés au moyen des différences partielles de $\Omega$ prises par rapport à $a$, $b$, $c$, \&c.,  et multipliées par des fonctions de ces constantes.  Nos formules ont l'avantage de donner immédiatement ces valeurs.»\end{quote}\noindent En effet,les formules de Poisson,données à la page 282 du mémoire en question,sont bien les coefficients $A$, $B$, $C$, $F$, \ldots\ de Lagrange,donnés comme fonctions des conditions initiales.Il faut noter qu'au passage,Poisson note les parenthèses de Lagrange par des crochets,et ses propres crochets par des parenthèses,ce qui ajoute à la confusion.La riposte de Lagrange ne tarde pas.Réalisant qu'il a laissé échapper un aspect relativement important de sa propre théorie,Lagrange publie quelques mois après un second mémoire {\em sur la théorie de la variation des constantes arbitraires dans les problèmes de mécanique}.Dans ce mémoire,lu à l'Institut le 19 février 1810,il donne sa version personnelle de l'inversion du système des parenthèses qui portent son nom.Il écrit dans l'introduction de ce mémoire \cite[p. 810]{Lagrange4}:\begin{quote}  « Mais l'application des formules générales aux Problèmes particuliers demandait encore un long calcul,  à cause des éliminations qu'il fallait faire pour obtenir séparément l'expression de la variation de chacune des constantes devenues variables.  Heureusement une considération très-simple,  que je vais exposer et qui m'avait échappé,  facilite et simplifie extrêmement cette application et ne laisse plus rien à désirer dans la Théorie analytique de la variation des constantes,  relativement aux questions de la Mécanique.»\end{quote}\noindent Toutefois,le mémoire de Lagrange ne se réduit pas à celui de Poisson,comme il l'écrit à la page 812:\begin{quote}  « M. Poisson a lu,  le 16 octobre dernier à cette Classe,  un {\em Mémoire sur la variation des constantes arbitraires dans les questions de Mécanique},  lequel est imprimé dans le volume qui vient de paraître du {\em Journal de l'\'Ecole Polytechnique}.  Ce Mémoire contient une savante analyse qui est comme l'inverse de la mienne,  et dont l'objet est d'éviter les éliminations que celle-ci exigeait.  L'Auteur parvient en effet,  par un calcul assez long et délicat,  à des formules qui donnent directement les valeurs des différentielles des constantes arbitraires devenues variables.  Ces formules ne coïncident pas immédiatement avec celles que je donne dans ce Mémoire,  parce qu'elles renferment les constantes arbitraires en fonction des variables du Problème et de leurs différentielles,  au lieu que les notres ne renferment ces constantes qu'en fonction d'autres constantes;  mais il est facile de se convaincre {\em à priori} qu'elles conduisent aux mêmes résultats.»\end{quote}\noindent Autrement dit,Lagrange exprime la matrice inverse de son système de parenthèses en termes d'un autre système de coordonnées de l'espace des solutions du problème.Ce qui,effectivement,ne coïncide pas immédiatement avec l'analyse de Poisson.Ce que publie Lagrange,c'est plutôt l'expression générale des {\em transformations canoniques}.Que faut-il alors retenir de ces deux points de vue,de Poisson et de Lagrange,de ces analyses qui,tout en étant différentes,sont si semblables?Voici quelques remarques qui,prises ensemble,peuvent expliquer le «retard» de Lagrange :\begin{enumerate}  \item Les parenthèses de Lagrange,  dans son premier mémoire de 1808,  sont les coordonnées covariantes de la forme symplectique;  les crochets de Lagrange,  dans son mémoire de 1810,  en sont les coordonnées contravariantes.  Qu'elles soient exprimées en fonction d'autres constantes du mouvement ou en tant que fonction des conditions initiales (temps, position, vitesse),  c'est en tant que coordonnées de la forme symplectique qu'elles sont exploitées pour étudier les problèmes des inégalités séculaires\footnote{L'apparition des crochets de Poisson comme méthode pour obtenir de nouvelles constantes du mouvement en fonction de certaines connues est légèrement postérieure.} (théorie des perturbations).  \item C'est la matrice des parenthèses,  fonction des constantes du mouvement,  qui est utile,  et utilisée (voir paragraphe \ref{parstabsec}),  pour l'intégration des perturbations.  Les crochets de Poisson sont difficilement exploitables pour cet usage puisqu'ils sont exprimés en fonction des conditions initiales.  \item En revanche,  si les parenthèses de Lagrange mettent en évidence la structure symplectique de l'espace des solutions des systèmes de la mécanique,  le théorème de Poisson met en évidence la structure d'algèbre de Lie des fonctions sur cette variété symplectique.\end{enumerate}Pourquoi l'histoire a-t-elle retenu davantage les crochets de Poisson plutôt que les parenthèses de Lagrange\footnote{Pour ajouter à la confusion,entre les premiers articles de Lagrange et sa {\em Mécanique Analytique} les parenthèses deviennent crochets et les crochets,parenthèses.Alors que dans l'article de Poisson,ce que nous appelons {\em crochets de Poisson\ } est noté par des parenthèses.Autrement dit,l'influence de Poisson sur Lagrange aura quand même conduit ce dernier à changer de notations.},tout au moins dans les manuels de mécanique?Peut-être parce que la seule façon de bien comprendre ces constructions de Lagrange c'est de les placer dans le cadre formel de la géométrie symplectique et que la géométrie de l'époque n'était pas encore prête?Peut-être aussi parce que,si la construction de Lagrange est bien adaptée aux systèmes dont on connait {\em a priori} l'intégration complète dans certaines conditions (on pourrait parler de {\em complète intégrabilité}),elle l'est moins lorsque l'on ne peut pas considérer le système réel comme une perturbation d'un système intégré.Mais,dans ces conditions,l'approche poissonienne est-elle mieux adaptée ?On peut réfléchir aussi à la question suivante :pourquoi a-t-il fallu plus d'un siècle et demi pour redécouvrir l'\oe uvre de Lagrange,effacée devant celle de ses épigones ?Je laisse là ces questions à la méditation du lecteur.\begin{note}  Siméon Denis Poisson est né à Pithiviers en 1781.  Il fut l'élève de Laplace et de Lagrange.  Il enseigna à l'École polytechnique à partir de 1802 et fut nommé astronome au bureau des longitudes en 1808.  Il meurt à Sceaux en 1840.\end{note}\begin{note}  Joseph-Louis Lagrange est né à Turin en 1736,  d'une famille originaire de France.  Il succède à Euler à Berlin en 1766,  et y reste jusqu'en 1787,  date à laquelle il vient à Paris en tant que membre vétéran de l'Académie des sciences.  Il publie en 1788\footnote{Prête semble-t-il en 1782.} la première version de sa {\em Mécanique Analytique},  puis publie en 1811 la première partie de la deuxième version qui contient pour la première fois des éléments de calcul symplectique.  Il meurt à Paris en 1813 et le deuxième volume de sa {\em Mécanique} n'est publié qu'après sa mort en 1816.\end{note}\noindent Enfin,je conseille vivement la lecture de l'ensemble des textes de Lagrange :trois mémoires déjà cités \cite{Lagrange2,Lagrange3,Lagrange4} et la {\em Mécanique Analytique} \cite{Lagrange1}.L'écriture de Lagrange est agréable,explicative et plus moderne qu'on ne peut l'imaginer {\em a priori}.En particulier,les introductions des chapitres sont de pures merveilles et des textes essentiels pour la compréhension de son \oe uvre et de l'histoire de la mécanique.Il vaut mieux éviter toutefois,en première lecture,les notes de Joseph Bertrand qui émaillent le texte de la {\em Mécanique Analytique} dans sa troisième édition.Elles portent sur des détails souvent peu importants et ne commentent pas les idées nouvelles qui y fourmillent.%%====================================================================% MARK: - Appendix B: Histoire d'H%%====================================================================\chapter{Histoire d'H}\label{AnnH}\noindent Dans cette annexe,j'essaye de faire le point sur les origines de la notation $H$ que l'on utilise ordinairement,dans les cours de mécanique,pour désigner le hamiltonien.Doit-on la considérer comme l'initiale de Hamilton ?Ou bien comme celle de Huygens ainsi qu'on peut le lire parfois\footnote{Plus précisément dans les divers textes de Souriau sur la mécanique \cite{Souriau9}.} ?Pour répondre à cette question il faut (re)lire Lagrange et Huygens ;on y découvre une partie du cheminement des idées qui ont conduit à cette découverte fondamentale du {\em principe de conservation des forces vives},connu aujourd'hui sous le nom de {\em théorème de conservation de l'énergie}.C'est le plaisir que j'ai eu à parcourir ces quelques pages d'histoire que j'aimerais partager ici.\section*{Le premier H}Dans les manuels de mécanique analytique,la lettre $H$ désigne traditionnellement une fonction appelée {\em hamiltonien} et qui représente l'énergie totale du système étudié,somme de son énergie cinétique et du potentiel des forces d'interaction.Cette notation est ainsi présentée comme un hommage des modernes à Sir Rowan Hamilton.Mais cette interprétation est erronée et résulte d'un malentendu historique.Ainsi que le fait remarquer Souriau dans ses divers textes sur la mécanique symplectique \cite{Souriau9},c'est Lagrange qui désigna pour la première fois,par la lettre $H$,la constante des forces vives\footnote{Si la {\em force vive} désigne le double de l'énergie cinétique,la {\em constante des forces vives},elle,représente l'énergie totale.},non comme un hommage à {\em Sir Rowan Hamilton} mais à {\em Christiaan Huyghens}.Que ces deux grands mathématiciens Huygens et Hamilton,partagent la même initiale a conduit à ce malheureux malentendu.Le {\em hamiltonien} devrait être ainsi renommé:{\em huyghensien}!Chacun peut vérifier que,dans sa {\em Mécanique Analytique},Lagrange désigne bien par la lettre $H$ la constante des forces vives.On peut y lire,à l'article 33,page 268 \cite[tome I, seconde partie, deuxième édition]{Lagrange1}:\addtolength{\baselineskip}{0.75ex}\begin{quote}  «L'équation précédente devient  $$    \SS\left({dxd^2x+dyd^2y+dzd^2z \over dt^2} +d\Pi\right)m = 0    $$  dont l'intégrale est  $$    \SS\left({dx^2+dy^2+dz^2 \over 2dt^2} +\Pi\right)m = \HH    $$  dans laquelle la lettre $\HH$ désigne une constante arbitraire,  égale à la valeur du premier membre de l'équation à un instant donné.  Cette dernière équation renferme le principe connu sous le nom de {\em Conservation des forces vives}.  En effet,  $dx^2+dy^2+dz^2$ étant le carré de l'espace que le corps parcourt dans l'instant $dt$,  $\frac{\raise0.5ex\hbox{{$\scriptstyle{dx^2+dy^2+dz^2}$}}} {\raise-0.5ex\hbox{{$\scriptstyle{dt^2}$}}}$ sera le carré de sa vitesse et $\frac{\raise0.5ex\hbox{{$\scriptstyle{dx^2+dy^2+dz^2}$}}} {\raise-0.5ex\hbox{{$\scriptstyle{dt^2}$}}}m$ sa force vive.  Donc $\SS\frac{\raise0.5ex\hbox{{$\scriptstyle{dx^2+dy^2+dz^2}$}}} {\raise-0.5ex\hbox{{$\scriptstyle{dt^2}$}}}m$ sera la somme des forces vives de tous les corps,  ou la force vive de tout le système;  et l'on voit par l'équation dont il s'agit,  que cette force vive est égale à la quantité $2\HH-2\SS\Pi m$,  laquelle dépend simplement des forces accélératrices qui agissent sur les corps,  et nullement de leur liaison mutuelle,  de sorte que la force vive du système est à chaque instant la même que les corps auraient acquises si,  étant animés des mêmes puissances,  ils s'étaient mus librement chacun sur la ligne qu'il a décrite.  C'est ce qui fait donner le nom de {\em Conservation des forces vives} à cette propriété du mouvement.»\end{quote}\addtolength{\baselineskip}{-0.5ex}\section*{Le principe des forces vives}\index{forces vives}C'est dans ce texte de sa {\em Mécanique Analytique},et non dans les mémoires fondateurs \cite{Lagrange2,Lagrange3,Lagrange4},que pour la première fois dans l'histoire de la mécanique la lettre $H$ désigna la {\em constante des forces vives},c'est-à-dire l'énergie totale du système.Nous continuons à observer cette convention mais avec une idée erronée de son origine.Le {\em Principe de conservation des forces vives} est connu aujourd'hui sous le nom de {\em Théorème de conservation de l'énergie};mais ce changement de dénomination cache un glissement sémantique,justement suggéré par l'apparition de cette constante.En effet,comme on le voit clairement dans l'équation précédente,et comme il l'est exprimé (suggéré sans être dit),la force vive n'est évidemment pas conservée!La tentative de justification de cette «conservation» reste confuse,et la question n'est vraiment réglée qu'avec l'introduction de la constante d'intégration $\HH$.D'autre part,à la date de parution de cet ouvrage,en 1811,Hamilton n'avait que cinq ans.Le fait est indéniable :$H$ n'est pas pour Hamilton,mais Lagrange n'explique pas,à cet endroit du texte (tome I, seconde partie, article 33),la raison de son choix.Il faut aller chercher ailleurs l' exposé de ses motivations,plus précisément dans l'introduction à la seconde partie de la mécanique,première section :{\em Sur les principes de la dynamique}.Outre son intérêt immédiat,sa lecture éclaire ce petit mystère.Voici ce qui est écrit à l'article 14 de la première section de la seconde partie du tome I de la mécanique de Lagrange :\begin{quote}  « Le premier de ces quatre principes,  celui de la conservation des forces vives,  a été trouvé par Huygens,  mais sous une forme un peu différente de celle qu'on lui donne présentement;  et nous en avons déjà fait mention à l'occasion du problème des centres d'oscillations.  Ainsi le principe de Huygens se réduit à ce que,  dans le mouvement des corps pesants,  la somme des produits des masses par les carrés des vitesses à chaque instant est la même soit que les corps se meuvent conjointement d'une manière quelconque,  ou qu'ils parcourent librement les mêmes hauteurs verticales.  Il [Huygens] donna ainsi à ce principe le nom de {\em conservation des forces vives},  et il s'en servit avec succès pour résoudre quelques problèmes qui ne l'avaient pas encore été,  et dont il paraissait difficile de venir à bout par des méthodes directes.»\end{quote}\section*{Le centre d'oscillations du pendule composé}Au nombre des problèmes résolus par Huygens grâce au principe de conservation des forces vives,le plus important pour notre propos est celui du {\em centre d'oscillations},qui lui fut posé par {\em le très savant Mersenne} (dixit Huygens \cite[page 90 de l'édition originale, citation extraite de la traduction française]{Huygens1}):\begin{quote}  Le très savant Mersenne me proposa jadis,  presqu'encore enfant,  et à de nombreux autres,  la recherche du Centre d'Oscillations ou d'Agitation,  question tout à fait fameuse parmi les Géomètres de ce temps\ldots\end{quote}\noindent Il s'agissait de déterminer le {\em pendule simple} isochrone à un {\em pendule composé},c'est-à-dire le pendule simple dont le battement est identique à celui d'un pendule composé donné.Laissons de nouveau à Lagrange le soin de nous exposer le problème [article 7 de la première section de la seconde partie du tome I de l'ouvrage sus-cité]:\begin{quote}  «Un fil considéré comme une ligne inflexible,  sans pesanteur et sans masse,  étant attaché par un bout à un point fixe,  et chargé à l'autre bout d'un petit poids qu'on puisse regarder comme réduit à un point,  forme ce qu'on appelle un pendule simple;  et la loi des vibrations de ce pendule dépend uniquement de sa longueur,  c'est-à-dire de sa distance entre le poids et le point de suspension.  Mais si à ce fil on attache encore un ou plusieurs poids à différentes distances du point de suspension,  on aura alors un pendule composé,  dont le mouvement devra tenir une espèce de milieu entre ceux des différents pendules simples que l'on aurait,  si chacun de ces poids était suspendu seul au fil.  Car la force de gravité tendant d'un côté à faire descendre tous les poids également dans le même temps,  et de l'autre l'inflexibilité du fil les contraignant à décrire des arcs inégaux et proportionnels à leur distance du point de suspension,  il doit se faire entre ces poids une espèce de compensation,  et de répartition de leurs mouvements,  en sorte que les poids qui sont les plus proches du point de suspension hâteront les vibrations des plus éloignés,  et ceux-ci,  au contraire,  retarderont les vibrations des premiers.  Ainsi il y aura dans le fil un point ou un corps étant placé,  son mouvement ne serait ni accéléré ni retardé par les autres poids,  mais serait le même que s'il était seul suspendu au fil.  Ce point sera donc le vrai centre d'oscillation du pendule composé,  et un tel centre doit se trouver dans tout corps solide de quelque figure que ce soit,  qui oscille autour d'un axe horizontal.»\end{quote}Le problème est bien posé,et il est d'importance puisque les horloges sont réglées justement grâce à des poids répartis sur leur balancier.Continuons notre lecture de Lagrange:\begin{quote}  «Huygens vit qu'on ne pouvait déterminer ce centre d'une manière rigoureuse sans connaître la loi suivant laquelle les différents poids du pendule composé altèrent mutuellement les mouvements que la gravité tend à leur imprimer à chaque instant;  mais au lieu de chercher à déduire cette loi des principes fondamentaux de la Mécanique,  il se contenta d'y suppléer par un principe indirect,  lequel consiste à supposer que si plusieurs poids attachés,  comme on le voudra,  à un pendule,  descendent par la seule action de la gravitation,  et que,  dans un instant quelconque,  ils soient détachés et séparés les uns des autres,  chacun d'eux,  en vertu de la vitesse acquise pendant sa chute,  pourra remonter à une telle hauteur,  que le centre commun de gravité se trouvera remonté de la même hauteur d'où il était descendu.»\end{quote}Voilà donc le principe,prémisse de la conservation des forces vives.Et Lagrange d'ajouter:\begin{quote}  «On ne saurait deviner ce qui a donné à l'auteur l'idée d'un tel principe  [\ldots]  Quoi qu'il en soit,  ce principe fournit une équation entre la hauteur verticale,  d'où le centre de gravité du système est descendu dans un temps quelconque,  et les différentes hauteurs verticales auxquelles les corps qui composent le système pourraient remonter avec leurs vitesses acquises,  et qui,  par le théorème de Galilée,  sont comme le carré de ces vitesses.»\end{quote}Voilà qui montre clairement comment la conservation des forces vives résout le problème du pendule composé.En vérité,le problème n'était pas encore considéré comme définitivement réglé.Les principes posés par Huygens n'étaient pas tout à fait ceux énoncés plus haut,et il fallait les fonder davantage.C'est Jacques Bernoulli qui définitivement satisfait les mécaniciens de l'époque.Mais tout cela est admirablement conté dans l'introduction à {\em La dynamique de la Mécanique} de Lagrange (articles 6 à 14) et nous y renvoyons le lecteur,pour son plaisir.\section*{Horologium Oscillatorium}Cela dit,avant de clore définitivement ce chapitre il nous faut encore préciser où et comment Huygens a présenté lui-même sa découverte.Pour cela,il faut lire la proposition V de la quatrième partie de l'{\em Horologium Oscillatorium} \cite[page~99 de l'édition originale, citation extraite de la traduction française]{Huygens1} :\begin{quote}  \centerline{\sc Proposition V}  \smallskip  « Étant donné un pendule composé des poids que l'on veut,  si chacun est multiplié par le carré de sa distance à l'axe d'oscillation,  et que la somme des produits soit divisée par le produit fait de la somme des poids par la distance au même axe d'oscillation du centre de gravité commun à tous;  il paraît la longueur du pendule simple isochrone au composé,  soit la distance entre l'axe et le centre d'oscillation même du pendule composé.»\end{quote}C'est dans la démonstration de cette proposition,qui donne la solution du problème du pendule composé,qu'apparaît,cachée mais bien là,la constante des forces vives.On peut lire en effet,à la page 100 de l'ouvrage sus-cité:\begin{quote}  « Mais comme le carré de la vitesse du point L qu'il possède en P est au carré de de la vitesse du point A en T,  ainsi est la hauteur à laquelle L peut monter par la première vitesse,  à la hauteur à laquelle A peut monter par la deuxième vitesse...»\end{quote}Accordons définitivement à Huygens le mérite de la découverte de la conservation de l'énergie (comme on nomme aujourd'hui ce principe).\section*{Conclusion}Ainsi donc le mystère de la lettre $H$ est éclairci,mais il est important de souligner qu'après les Huygens,Bernoulli et autres,celui qui a donné son sens moderne à ce principe,qui l'a établi dans toute sa rigueur analytique,c'est Lagrange.Et c'est,encore aujourd'hui,la même façon qu'il a eu de le démontrer,il y a deux cent ans,que ce principe est enseigné dans nos facultés.%%====================================================================% MARK: - Appendix C: Éléments de géométrie symplectique%%====================================================================\chapter{Éléments de géométrie symplectique}\label{geomsymp}\noindent Cette annexe est destinée à présenter les éléments essentiels fondant la géométrie symplectique moderne.Ce n'est toutefois pas un cours de géométrie symplectique;bien entendu le lecteur peut se reporter aux ouvrages classiques cités en référence pour un exposé systématique.Pour ce qui suit,nous considérons le lecteur familier à la fois avec l'algèbre linéaire ordinaire et les éléments de base de géométrie différentielle.\section*{Espaces vectoriels symplectiques}Soit $E$ un espace vectoriel réel de dimension finie.On appelle {\em forme bilinéaire alternée}\index{forme bilinéaire} (ou antisymétrique) de $E$ toute application\begin{equation}  \omega : E\times E\to \RR\end{equation}telle que:\begin{eqnarray}  \omega(u+v,w) & = & \omega(u,w)+\omega(v,w)\nonumber \\  \omega(\lambda u,v) & = & \lambda\omega(u,v) \\  \omega(u,v) & = & -\omega(v,u),\nonumber\end{eqnarray}pour tout $u,v,w$ vecteurs de $E$,et $\lambda$ réel.On appelle {\em noyau} de la forme $\omega$ le sous-espace vectoriel $\ker \omega\subset E$ défini par\begin{equation}  \ker \omega = \{ u\in E \mid \omega(u,v)=0 \quad \forall v\in E\}.\end{equation}On considère en même temps la forme $\omega$ comme une application linéaire de $E$ dans son dual grâce à l'identification:\begin{equation}  \begin{array}{rrcl}    \omega : & E & \longrightarrow & E^* \\    & u & \longmapsto     & \bar u : v\mapsto \omega(u,v)  \end{array}\end{equation}On note aussi $\bar u = \omega(u,\cdot)$ ou encore $\bar u=\omega(u)$,de sorte que $\omega(u,v)=\bar u(v) =\omega(u)(v)$.Le noyau de $\omega$ est le noyau de cette application linéaire.\begin{definition}  Étant donnée une forme bilinéaire alternée $\omega$ définie sur l'espace vectoriel $E$,  on dit qu'elle est {\em symplectique} si elle est non dégénérée,  \cad de noyau nul:  $\ker\omega=\{0\}$.\end{definition}Puisque nous avons une application bilinéaire sur $E$,nous avons par la même occasion les notions d'orthogonalité,de sous-espace orthogonal à un sous-espace vectoriel donné:\begin{definition}  Étant donnés deux vecteurs $u$ et $v$ de $E$,  nous disons qu'ils sont orthogonaux pour $\omega$ (ou tout simplement orthogonaux) si $\omega(u,v)=0$.  Étant donné un sous-espace $V\subset E$,  nous appelons sous-espace orthogonal à $V$ l'ensemble:  $$    \Orth(V) = \{ u\in E \mid \omega(u,v)=0 \quad \forall v\in V\}.    $$\end{definition}Cela permet de réinterpréter le noyau $\ker \omega$ comme l'orthogonal de $E$ tout entier:$\ker \omega =\Orth(E)$.Dans ce point de vue,$\omega$ est symplectique si $\Orth(E)=\{0\}$.L'orthogonalité d'une forme bilinéaire alternée s'accompagne naturellement des notions d'{\em isotropie} et de {\em co-isotropie}:\begin{definition}  Nous disons qu'un sous-espace vectoriel $V$ de $E$ est {\em isotrope}\index{isotrope} s'il est contenu dans son orthogonal\index{orthogonal symplectique}:  $V\subset\Orth(V)$.  Nous disons qu'il est {\em co-isotrope}\index{co-isotrope} s'il contient son orthogonal:  $\Orth(V)\subset V$.\end{definition}À titre d'exemples:le noyau de $\omega$ est évidemment isotrope;toute droite vectorielle est isotrope (appliquer l'antisymétrie);tout hyperplan est co-isotrope.Considérons la restriction de $\omega$ sur un sous-espace vectoriel $V$,son noyau est donné par:\begin{equation}  \ker\omega\vert_{V}=\Orth(V)\cap V.\end{equation}L'espace quotient $V/\ker\omega\vert_V=V/(\Orth(V)\cap V)$ est donc muni naturellement par projection d'une forme symplectique.On appelle cette méthode de construction d'espace vectoriel symplectique la {\em méthode de réduction symplectique}\index{réduction symplectique}.Lorsque $V$ est co-isotrope alors $\Orth(V)\cap V=\Orth(V)$ et le quotient est simplement $V/\Orth(V)$.On obtient encore,comme conséquence de cette dernière égalité,que si la restriction de $\omega$ sur $V$ est symplectique alors $V$ et $\Orth(V)$ sont complémentaires,et même supplémentaires compte tenu du théorème suivant:\begin{theoreme}  Soit $E$ un espace vectoriel muni d'une forme symplectique $\omega$,  soit $V$ un sous-espace vectoriel quelconque de $E$,  alors:  \begin{equation}    \dim V + \dim\Orth(V) =\dim E.  \end{equation}\end{theoreme}\begin{demonstration}  Soit $\cV=(v_1,\ldots,v_r)$ une base de $V$,  $\dim V= r$.  Considérons l'application linéaire $\bar\cV:E\to \RR^r$ définie par:  \begin{equation}    \bar\cV(w)=(\bar v_1(w),\ldots,\bar v_r(w)).  \end{equation}  En remarquant que le noyau de cette application est naturellement l'orthogonal symplectique de $V$ et en appliquant le théorème classique d'algèbre linéaire:  \begin{eqnarray*}    \dim E & = & \dim\im \bar\cV + \dim \ker\bar\cV \\    & = & \dim V + \dim \Orth(V),  \end{eqnarray*}  on établit l'égalité annoncée.\end{demonstration}Mais attention,cela ne veut pas dire que pour tout sous-espace $V$,$V$ et $\Orth(V)$ sont supplémentaires;par exemple,si $V$ est une droite,son orthogonal est un hyperplan contenant $V$.Cette formule indique aussi que la dimension d'un espace isotrope (ou co-isotrope) est nécessairement inférieure,ou égale,à la moitié de la dimension de l'espace.Ce qui conduit à la définition suivante:\begin{definition}  On appelle sous-espace vectoriel {\em lagrangien} tout sous-espace vectoriel isotrope saturé\footnote{C'est sous ce nom:  {\em Variétés isotropes saturées} que les {\em variétés lagrangiennes} (sous-variétés tangentes en chaque point à un espace vectoriel lagrangien) sont apparues originellement en géométrie symplectique \cite{Souriau20}.}.\end{definition}La dimension d'un espace lagrangien est nécessairement de dimension moitié de celle de l'espace symplectique comme l'implique le théorème fondamental suivant:\begin{theoreme}  [{\sc de la base canonique}]\index{base canonique}  Soit $\omega$ une forme bilinéaire alternée définie sur un espace vectoriel $E$,  il existe une base  $$    S=(u_1,\ldots,u_n,v_1,\ldots,v_n,k_1,\ldots,k_r)    $$  de $E$ telle que:  \begin{enumerate}    \item $\ker\omega = \left<k_1,\ldots,k_r\right>$,    \item $\omega(u_i,u_j)=0$,    $\omega(v_i,v_j)=0$ et $\omega(u_i,v_j)=\delta_{ij}$,  \end{enumerate}  où $\delta$ est le symbole de Kronecker et les symboles $\left<\cdots\right>$ désignent le sous-espace vectoriel engendré.  Une telle base est appelée {\em base canonique}.\end{theoreme}\begin{demonstration}  Soit $k_1,\ldots,k_r$ une base de $\ker\omega$ et $F$ un supplémentaire de $\ker\omega$.  Alors $\omega\vert_F$ est symplectique et on peut compléter $k_1,\ldots,k_r$ par une base de $F$ pour en faire une base de $E$.  Le problème se ramène donc au cas symplectique.  Supposons donc que $E$ soit symplectique.  Soit $u_1$ un vecteur non nul,  il existe alors un vecteur $v_1$ tel que $\omega(u_1,v_1)\neq 0$,  sinon $u_1$ serait dans le noyau de $\omega$.  On peut alors choisir $v_1$ pour que $\omega(u_1,v_1)=1$.  Soit $E_2$ l'orthogonal symplectique du sous-espace vectoriel engendré par le couple $(u_1,v_1)$,  $E_2=\Orth(\left<u_1,v_1\right>)$.  Les espaces $E_2$ et $\left<u_1,v_1\right>$ sont supplémentaires et la restriction de $\omega$ à $E_2$ est donc symplectique,  comme nous l'avons vu plus haut.  Il suffit donc d'appliquer cette procédure un nombre fini de fois puisque la dimension de $E=E_1$ est finie et qu'à chaque étape la dimension des $E_i$ successifs décroit de 2.  Il suffit alors de vérifier que les $u_i$ et $v_i$ ainsi choisis vérifient bien les conditions du théorème.\end{demonstration}\noindent On remarque que si $E$ est symplectique,sa dimension est paire;autrement dit,le rang d'une forme antisymétrique est pair.Dans une base canonique,la matrice de Gramm $\Omega$ de la forme $\omega$ s'écrit:\begin{equation}  \Omega = \left(  \begin{array}{r@{}cc}    & {\bf 0_n} & {\bf 1_n} \\    - & {\bf 1_n} & {\bf 0_n} \\  \end{array}  \right).\end{equation}\begin{exemple}  L'exemple standard d'espace vectoriel symplectique est $\RR^{2n}$:  soient  $$    X=(u,v) \mbox{ et }X'=(u',v')    $$  deux vecteurs de $\RR^{2n}=\RR^n \times\RR^n$,  la forme symplectique canonique $\omega$ est définie par  \begin{equation}    \omega(X,X')=u.v'-u'.v,  \end{equation}  où le point désigne le produit scalaire;  autrement dit:  \begin{equation}    \omega(X,X')=\sum_{i=1}^n u_iv'_i-u'_i.v_i.  \end{equation}  La base canonique $(e_1,\ldots,e_n,e_{n+1},\ldots,e_{2n})$ est symplectique.  La forme symplectique $\omega$ s'appelle la {\em forme symplectique canonique}\index{forme canonique} de $\RR^{2n}$.\end{exemple}\begin{remarque}  On peut généraliser certaines de ces constructions à la dimension infinie mais il faut faire attention à choisir les bonnes définitions.  Pour le lecteur averti,  voici un exemple amusant qui mériterait de plus amples développements.  On considère une surface compacte orientée $\Sigma$,  %% TODO: The user will provide the modern image file for this figure.  %% \begin{figure}[ht]  %% \centerline{\includegraphics{figures/fig-surface.pdf}}  %% \caption{Surface de genre 3 et base symplectique de l'homologie $(A_i,B_i)$}  %% \label{surface}  %% \end{figure}  de genre $g$.  On considère l'espace vectoriel $E$ de ses 1-formes différentielles.  C'est un espace de dimension infini.  On définit sur cet espace vectoriel la forme bilinéaire suivante:  \begin{equation}    \omega(a,b) = \int_\Sigma a\wedge b.  \end{equation}  Cette définition a bien un sens puisque:  1) la surface $\Sigma$ est compacte et donc l'intégrale converge;  2) la surface $\Sigma$ est orientée.  La forme bilinéaire $\omega$ est antisymétrique,  conséquence des lois du produit extérieur.  Elle est non dégénérée dans le sens ordinaire:  \begin{equation}    \omega(a,b) = \int_\Sigma a\wedge b = 0 \quad \forall b \quad \Rightarrow a=0.  \end{equation}  On peut donc dire que $\omega$ est une forme symplectique sur $E$.  Il n'est pas question d'établir dans ce cas le théorème de la base symplectique,  tout au plus peut-on espérer montrer l'existence de sous-espaces vectoriels,  isotropes,  co-isotropes et finalement lagrangiens.  Considérons le sous-espace vectoriel $B$ des formes différentielles exactes (dérivées extérieures de fonctions réelles).  Cet espace est isotrope,  en effet,  pour tout couple $f$ et $g$ de fonctions:  \begin{equation}    \omega(df,dg) = \int_\Sigma df\wedge dg = \int_\Sigma d(f dg)=\int_{\partial\Sigma} f dg =0.  \end{equation}  Montrons que son orthogonal symplectique est le sous-espace des formes différentielles fermées $Z$ de $\Sigma$:  \begin{equation}    \omega(df,a) = \int_\Sigma df\wedge a = \int_\Sigma d(f a)- \int_\Sigma f da=-\int_\Sigma f da;  \end{equation}  donc $\omega(df,a)=0$ pour tout $f$ implique $\int_\Sigma f da=0$,  \cad $da=0$,  ce qui se résume ainsi:  \begin{equation}    \Orth(B)=Z \quad \Leftrightarrow \quad \Orth(Z)=B.  \end{equation}  Nous avons vu plus haut que la méthode de réduction symplectique munit le quotient d'un espace co-isotrope par son orthogonal d'une structure symplectique canonique.  Ici,  le sous-espace co-isotrope choisi est $Z$ et le quotient symplectique $Z/B$ est le premier espace de cohomologie de De Rham $H^1(\Sigma,\RR)$:  \begin{equation}    Z/(B=\Orth(Z))= H^1(\Sigma,\RR).  \end{equation}  Nous savons par ailleurs,  grâce à la théorie des surfaces,  que cet espace est de dimension finie:  $\dim H^1(\Sigma,\RR)=2g$,  autrement dit $Z/(B=\Orth(Z))\simeq \RR^{2g}$.  Nous savons que $\RR^{2g}$ possède des espaces vectoriels lagrangiens,  que l'on peut d'ailleurs tous construire grâce à la méthode de la base symplectique.  Soit $L$ un de ces espaces et $\tilde L$ sa pré-image par la projection canonique $Z\to H^1(\Sigma,\RR)$.  Compte tenu de ce qui précède,  le lecteur peut vérifier lui-même que ce sous-espace vectoriel $\tilde L$ est lagrangien,  \cad que c'est un sous-espace vectoriel isotrope saturé de l'espace $E$ des 1-formes différentielles sur $\Sigma$.\end{remarque}Il ne saurait être question de géométrie symplectique sans l'introduction des «{\em groupes symplectiques}».\begin{definition}  On appelle {\em groupe symplectique} de l'espace vectoriel symplectique $(E,\omega)$ le sous-goupe $\Sp(E,\omega)$ du groupe linéaire $\GL(E)$ des éléments préservant $\omega$,  \cad:  \begin{equation}    \Sp(E,\omega) = \{ A\in \GL(E) \mid \omega(Au,Av)=\omega(u,v) \quad \forall (u,v)\in E\times E \}.  \end{equation}  Ce groupe est noté $\Sp(2n,\RR)$ lorsque $E=\RR^{2n}$ muni de sa forme symplectique canonique.\end{definition}Il est bien évident que les groupes symplectiques d'espaces vectoriels symplectiques de même dimension sont isomorphes,puisque les espaces vectoriels symplectiques le sont eux-mêmes,d'après le théorème de la base symplectique.Il est inutile de préciser que ce groupe joue un rôle fondamental en géométrie symplectique,et nous renvoyons aux ouvrages spécialisés pour plus de détails.Venons en à la {\em géométrie différentielle symplectique}.La définition d'une {\em variété symplectique}\index{variété symplectique} copie la définition des espaces vectoriels symplectiques dans le sens qu'une structure symplectique sur une variété est définie par la donnée sur chaque espace tangent d'une forme symplectique,à ceci près que l'on demande,en plus,à cette forme d'être fermée.Donnons la définition formelle:\begin{definition}  Soit $X$ une variété différentielle,  on appelle {\em forme symplectique} sur $X$ toute 2-forme différentielle $\omega$ non dégénérée fermée:  $$    \forall x\in X \quad \ker \omega_x =\{0\} \quad \mbox{ et } \quad d\omega =0.    $$  Une variété munie d'une forme symplectique s'appelle {\em variété symplectique}.\end{definition}Dans cette définition,$\omega_x$ désigne la valeur de la forme $\omega$ au point $x$,c'est donc une forme bilinéaire antisymétrique définie sur l'espace vectoriel tangent $T_xX$,et $d\omega$ désigne la dérivée extérieure de $\omega$.Pour ces définitions de géométrie différentielle ordinaire,nous renvoyons le lecteur aux ouvrages classiques,ou bien à l'annexe suivante \ref{AnnED} pour la notion de différentielle extérieure.\begin{exemple}  L'exemple classique de variété différentielle est celui de l'espace cotangent $T^*Q$ à une variété quelconque $Q$.  Nous avons déjà rencontré cet exemple au chapitre \ref{parHomLag}.  Nous n'y reviendrons pas.\end{exemple}\begin{note}  Les notions de {\em sous-variété isotrope},  {\em sous-variété co-isotrope},  {\em sous-variété lagrangienne} se généralisent naturellement à partir de l'algèbre linéaire:  ce sont des sous-variétés (plongée ou immergée, suivant les cas) qui sont tangentes en chaque point à un sous-espace vectoriel,  respectivement,  isotrope,  co-isotrope ou lagrangien de l'espace tangent.\end{note}Le théorème essentiel de la géométrie symplectique est le théorème suivant,dû à Gaston Darboux,qui dit que toute variété symplectique est non seulement infinitésimalement isomorphe à $\RR^{2n}$ canonique,comme nous pourrions nous y attendre,mais même {\em localement} isomorphe,\cad sur un voisinage entier et non seulement en un point.\begin{theoreme}  [{\sc Darboux}]\index{théorème de Darboux}  Soit $(X,\omega)$ une variété symplectique de dimension $2n$.  Soit $x_0\in X$,  il existe une carte $F:U\to X$,  où $U$ est un voisinage de $0\in\RR^{2n}$,  envoyant $0$ sur $X_0$,  et telle que $F^*(\omega)$ coïncide,  sur son domaine de définition,  avec la forme symplectique canonique de $\RR^{2n}$.  Une telle carte est appelée {\em carte de Darboux}.\end{theoreme}\noindent Pour démontrer ce théorème fondamental,nous utiliserons le suivant,dû à Moser:\begin{theoreme}  [{\sc Moser}]  Soit $X$ une variété symplectique compacte (sans bord),  soient $\omega$ et $\omega'$ deux formes symplectiques sur $X$ homotopes à travers la même classe de cohomologie.  Il existe alors un difféomorphisme $\phi$ de $X$,  homotope à l'identité,  tel que $\phi^*\omega'=\omega$.\end{theoreme}\noindent Nous admettrons le lemme suivant\begin{lemme}  Soit $X$ une variété compacte (sans bord).  Soit $[t\mapsto \omega_t]$ un arc différentiable dans l'espace des p-formes exactes,  dans le sens suivant\footnote{C'est la différentiabilité au sens des {\em espaces différentiables},  voir annexe \ref{AnnED}.} :  l'application $[(t,x)\mapsto \omega_t(x)]$ est différentiable sur un voisinage ouvert de $\times X$ dans l'espace des p-formes et prend ses valeurs dans le sous-espace des formes fermées exactes.  Il existe alors un arc différentiable $[t\mapsto \beta_t]$ de (p-1)-formes tel que pour tout $t$:  $\omega_t = \omega_0+d\beta_t.$\end{lemme}\begin{demonstration}  {\sc (Théorème de Moser)}\index{théorème de Moser}  On suppose l'homotopie $[t\mapsto\omega_t]$,  définie sur l'intervalle $]-\varepsilon,1+\varepsilon[$,  $\varepsilon>0$,  ce qui est une conséquence de la différentiabilité de l'arc et notons:  $$    \omega_t = \omega_0+d\beta_t,    $$  où $[t\mapsto\beta_t]$ est un arc différentiable de 1-formes (d'après le lemme précédent).  Soit $Y$ la variété $]-\varepsilon,1+\varepsilon[\times X$ sur laquelle on prolonge chaque 2-forme $\omega_t$ par zéro sur $dt$;  soit $\Omega$ ce prolongement:  $$    \Omega_{(t,x)}\left((s , u),(s' , u')\right) = (\omega_t)_x(u,u').    $$  Considérons le champ de vecteurs $\Xi$ défini sur $Y$ par:  $$    \Xi\vect{t\\ x} = \vect{1 \\ \xi_t(x)}\mbox{ avec } \xi_t(x)= -(\omega_t^{-1})_x\left({\partial \beta_t(x)\over \partial t}\right).    $$  Pour tout $t$ dans l'intervalle $]-\varepsilon,1+\varepsilon[$,  notons $j_t$ l'injection $x\mapsto(t,X)$ de $X$ dans $\{t\}\times X$.  Démontrons que l'image réciproque de la dérivée de Lie $\DLie_\Xi\Omega$ par $j_t$ est nulle pour tout $t$.  Soient $x\in X$,  $u$ et $u'$ deux vecteurs tangents en $x$ à $X$.  Nous avons:  $$    j_t^*(\DLie_\Xi\Omega)_x(u,u') =\DLie_\Xi\Omega_{(t,x)}\vect{0\\ u}\vect{0\\ u'}.    $$  Appliquons la formule de Cartan:  $\DLie_\Xi\Omega=d\Omega(\Xi) + d(\Omega(\Xi))$,  et calculons les deux termes de droite de l'égalité suivante:  $$    j_t^*(\DLie_\Xi\Omega)_x(u,u') =d\Omega(\Xi)_{(t,x)}\vect{0\\ u}\vect{0\\ u'}+ d(\Omega(\Xi))_{(t,x)}\vect{0\\ u}\vect{0\\ u'}.    $$  Pour cela calculons d'abord $d\Omega$.  Exprimons $\Omega$ dans une carte adaptée:  $$    \Omega_{(t,x)}=\sum_{i<j}(\omega_t)_{ij}(x)dx_i\wedge dx_j.    $$  On déduit la dérivée extérieure  $$    d\Omega_{(t,x)}=d\omega_t + \sum_{i<j}{\partial(\omega_t)_{ij}(x)\over \partial t} dt\wedge dx_i\wedge dx_j= d\omega_t+dt\wedge{\partial\omega_t(x)\over \partial t}    $$  mais $\omega_t$ étant fermée,  par hypothèse,  il reste:  $$    d\Omega_{(t,x)}=dt\wedge{\partial\omega_t(x)\over \partial t}.    $$  Contracté avec $\Xi$ et appliqué au couple de vecteurs $(0,u)$,  $(0,u')$,  il vient le premier terme de $j_t^*(\DLie_\Xi\Omega)_x(u,u')$:  \begin{eqnarray*}    d\Omega_{(t,x)}\left(\Xi(t,x)\right)\vect{0\\ u}\vect{0\\ u'} & = & d\Omega_{(t,x)}\vect{1\\ \xi_t(x)}\vect{0\\ u}\vect{0\\ u'} \\    & = & {\partial \omega_t(x)\over \partial t}(u,u').  \end{eqnarray*}  D'autre part,  pour tout vecteur $(s,u)$ tangent à $Y$ au point $(t,x)$,  nous avons:  \begin{eqnarray*}    \Omega_{(t,x)}\left(\Xi(t,x)\right)\vect{s \\ u} & = & \Omega_{(t,x)}\vect{1\\ \xi_t(x)}\vect{s \\ u} \\    & = & (\omega_t)_x(\xi_t(x))(u) \\    & = & (\omega_t)_x \left(-(\omega^{-1}_t)_x\left({\partial \beta_t(x)\over \partial t}\right)\right)(u) \\    & = & -{\partial \beta_t(x) \over \partial t}(u).  \end{eqnarray*}  On en déduit,  puisque le terme de dérivée seconde en $t$ n'intervient pas,  que:  \begin{eqnarray*}    d(\Omega(\Xi))\vect{0 \\ u}\vect{0\\ u'} & = & -d\left({\partial \beta_t\over \partial t}\right)_x(u,u') \\    & = & -{\partial (d\beta_t)_x\over \partial t}(u,u').  \end{eqnarray*}  On a obtenu ainsi les deux termes de la dérivée de Lie recherchée:  \begin{eqnarray*}    j_t^*(\DLie_\Xi\Omega)_x(u,u') & = & {\partial \omega_t(x)\over \partial t}(u,u') - {\partial (d\beta_t)_x\over \partial t}(u,u') \\    & = & {\partial \over \partial t}\bigg[(\omega_t)_x(u,u') - (d\beta_t)_x(u,u')\bigg].  \end{eqnarray*}  En comparant avec l'hypothèse $\omega_t =\omega_0 + d\beta_t$,  on conclut:  $$    j_t^*(\DLie_\Xi\Omega)_x =0.    $$  Autrement dit,  l'image réciproque de la forme $\Omega$ par le flot de $\Xi$ et $\Omega$ coïncident en restriction à chaque sous-variété $\{t\}\times X$.  On a alors en particulier:  $$    j_0^*\left(\left(e^{s\Xi}\right)^*\Omega\right)= \left(e^{s\Xi}\circ j_0\right)^*\Omega = j_0^*\Omega.    $$  On reconnait $\omega_0$ dans le terme de droite.  D'autre part $e^{s\Xi}\circ j_0$ est un difféomorphisme $\phi_s$ de $X$ sur $\{s\}\times X\simeq X$,  donc:  $$    (e^{s\Xi}\circ j_0)^*\Omega =\phi_s^*(\Omega\vert_{\{s\}\times X}) = \phi_s^*(\omega_s).    $$  Enfin,  le champ de vecteurs $\Xi$ est complet sur le produit $\times X$,  grâce à la compacité de $X$ et parce qu'il est transverse aux fibres $\{t\}\times X$.  Il échange en particulier les fibres $\{t\}\times X$,  plus précisément $e^{s\Xi}(\{t\}\times X)= \{t+s\}\times X$.  Nous pouvons alors choisir $s=1$;  le difféomorphisme $e^\Xi$ envoie $\{0\}\times X$ sur $\{1\}\times X$ et nous avons le résultat attendu:  $\phi^*\omega_1=\omega_0$.\end{demonstration}\noindent Venons en à la démonstration du théorème de Darboux:\begin{demonstration}  {\sc (Théorème de Darboux)}  Soit $F:U\to X$ une carte telle que $F(0)=x_0$,  l'image réciproque $F^*\omega$ est une forme symplectique définie sur un ouvert $U$ de $\RR^{2n}$.  Le problème est donc ramené au cas où $X=U$,  ce que nous supposons maintenant.  Il s'agit alors de trouver un difféomorphisme $\phi$ d'un voisinage $V\subset U$ qui fixe $0$ et qui transforme la forme symplectique $\omega$ en la forme canonique sur tout $V$.  On peut déjà supposer que la valeur $\omega_0=\omega(0)$,  est la forme canonique,  en appliquant le théorème de la base symplectique.  Considérons le chemin $t\mapsto \omega_t =t\omega+(1-t)\omega_0$ qui joint $\omega_0$,  défini sur tout $U$,  et $\omega$.  Comme $(\omega_t)(0) = t\omega(0) + (1-t)\omega_0(0) = \omega_0$ pour tout $t$,  il existe une boule $B$,  voisinage de $0$ dans $U$,  telle que $\omega_t$ soit symplectique sur $B$ pour tout $t$ dans un certain voisinage de $B$.  En effet,  la condition de fermeture est évidemment vérifiée.  Reste la régularité,  qui s'exprime par la condition:  $\det \Omega_t(x) \neq 0$.  Mais le déterminant étant continu,  puisque $\det (\Omega_t)(0)\neq 0$ pour tout $t$,  il existe (pour tout $t$) une boule $B_t\subset\RR\times V$,  centrée en $(t,0)$,  telle que $\det \Omega_t(x) \neq 0$ sur $B_t$.  On a donc un recouvrement ouvert de $\times\{0\}$.  Par compacité on en extrait un recouvrement fini $(B_i)$,  et il suffit ensuite de prendre l'intersection $B$ des boules $B_i\cap (\{i\}\times U)$.  Nous sommes ainsi ramené à l'énoncé du théorème de Moser,  à ceci près que la boule $B$ n'est pas une variété compacte et que nous ne pouvons pas invoquer sa compacité pour en déduire la complétude du champ de vecteur $\Xi$ de la démonstration précédente,  défini sur $\times B$ (le rôle de la variété $X$ est maintenant joué par $B$).  Mais la complétude du champ $\Xi$,  condition suffisante,  n'est pas nécessaire dans notre cas,  il suffit que nous connaissions la courbe intégrale du champ de vecteur $\Xi$ passant par le point $(0,0)\in\times B$,  c'est-à-dire l'application $s\mapsto e^{s\Xi}(0,0)=(s,0)$,  qui envoie le point $(0,0)$ sur $(1,0)$.  En appliquant alors le théorème de Cauchy sur les équations différentielles ordinaires,  on montre qu'il existe une sous-variété transverse $W=\{0\}\times B'$ à $\Xi$ telle que l'exponentielle restreinte à $W$ soit un difféomorphisme de $W$ sur son image $e^{s\Xi}(W)\subset\{1\}\times B$ qui est un voisinage de $0$ dans $B$.  La démonstration du théorème de Moser s'applique alors sans modification formelle à cette situation.\end{demonstration}\begin{remarque}  Toute variété symplectique $(X,\omega)$ possède un volume \footnote{Toute variété symplectique est donc orientable et naturellement orientée.},  il suffit de choisir $\vol = \omega^n$,  où $\dim X=2n$.  Réciproquement,  toute variété orientable de dimension paire n'est pas nécessairement symplectique.  Pour les sphères de dimension paire par exemple,  seule la sphère $S^2$ est symplectique.  En effet,  toute variété compacte $X$ symplectique a son deuxième groupe d'homologie $H_2(X,\ZZ)$ non nul :  si l'on suppose le contraire alors $\omega=d\alpha$ donc $\vol=d(a\wedge \alpha^{n-1})$ et $\int_X\vol=0$ ce qui est absurde.  La seule sphère ayant son deuxième groupe d'homologie non nul est $S^2$.  Remarquons à ce propos que la seule condition pour qu'une surface (variété de dimension 2) soit symplectique est d'être orientable.  Mais le cas des surfaces est plutôt un cas dégénéré de la géométrie symplectique.  Les théorèmes symplectiques y sont soit faux soit triviaux\footnote{Selon une expression que j'aime beaucoup de Michèle Audin.}\end{remarque}\begin{remarque}  Le théorème de Darboux montre que la géométrie symplectique différentielle n'a pas d'invariants locaux,  comme c'est le cas pour d'autres types de géométrie différentielle --- par exemple la géométrie riemannienne où l'on a à sa disposition la courbure et autres caractéristiques de ce type.  Toute structure symplectique est localement {\em plate}.  Les seuls invariants symplectiques sont donc des invariants globaux.  Par exemple le {\em nombre de Darboux} d'une variété symplectique est le nombre minimal de cartes nécessaires à un atlas de Darboux,  un atlas de Darboux étant évidemment un atlas constitué exclusivement de cartes de Darboux.  Mais ce nombre est souvent difficile à calculer explicitement.  Un des jeux des géomètres symplecticiens est d'inventer justement de tels invariants globaux et d'essayer ensuite de les calculer (ou de les faire calculer par leurs élèves).  C'est exactement cela qui rend la géométrie différentielle symplectique contradictoire:  elle est à la fois facile,  localement tout se ressemble,  et difficile du point de vue de la structure globale.\end{remarque}Il n'est pas possible d'achever ce paragraphe sans évoquer à la fois le groupe des {\em symplectomorphismes} d'une variété symplectique $(X,\omega)$ et la notion de gradient symplectique d'une fonction réelle $f$,notions qui sont à la base de la géométrie différentielle symplectique.\begin{definition}  On appelle {\em symplectomorphisme} d'une variété symplectique $(X,\omega)$ tout difféomorphisme $\phi$ de $X$ tel que $\phi^*\omega=\omega$.  L'ensemble des symplectomorphismes de $X$ constitue le {\em groupe des symplectomorphismes} de $X$,  qui est noté $\Sympl(X)$.\end{definition}La forme symplectique $\omega$ au point $x$ réalise un isomorphisme entre $T_xX$ et $T^*_xX$.Son inverse,noté $\omega_x^{-1}$,est l'isomorphisme qui permet la définition du {\em gradient symplectique}.\begin{definition}  Soit $f$ une fonction réelle de $X$,  sa dérivée extérieure $df$ et,  en chaque point $x$,  un covecteur:  $df(x)\in T^*_xX$.  On appelle {\em gradient symplectique}\index{gradient symplectique} de $f$ le champ de vecteurs sur $X$ défini en tout point $x$ par :  $$    \grad_\omega(f)(x) =\omega^{-1}_x(df(x)).    $$\end{definition}Il n'est pas dans les objectifs de cet ouvrage d'énoncer toutes les vertus et propriétés du gradient symplectique,mais celle-ci est importante:\begin{proposition}  Soit $(X,\omega)$ une variété symplectique compacte et $f$ une fonction différentiable réelle sur $X$,  alors son gradient symplectique est complet et son exponentielle $\exp(s\grad_\omega f)$ est un symplectomorphisme pour tout $s$.\end{proposition}Cette proposition (dont on laisse la démonstration au lecteur) montre clairement que le groupe des symplectomorphismes\index{symplectomorphisme} est de dimension infinie.$$  \dim\Sympl(X,\omega)=\infty.  $$\begin{remarque}  Il est tentant de vouloir comparer la géométrie symplectique et la géométrie riemannienne.  Mais cette comparaison est déraisonnable.  Même si l'on est tenté de jouer sur l'aspect contradictoire {\em tenseur symétrique} versus {\em tenseur anti-symétrique},  cette comparaison rejette dans l'ombre un aspect essentiel de la structure symplectique qui n'a pas son équivalent côté riemannien:  la {\em fermeture} de la forme symplectique $\omega$,  \ie $d\omega=0$.  Cette propriété rapproche davantage la géométrie symplectique de la géométrie des {\em 1-tenseurs} que de la géométrie riemannienne.  Bien que cela n'en était pas l'objectif principal,  c'est ce que j'ai montré,  plus ou moins,  tout au long de ce livre.  Il est donc inutile d'insister sur l'aspect $\dim \Sympl(X,\omega)=\infty$ versus $\dim\Isom(X,g)<\infty$,  où $g$ est une métrique riemannienne $X$.\end{remarque}%%====================================================================% MARK: - Appendix D: Espaces différentiables%%====================================================================\chapter{Espaces différentiables}\label{AnnED}\index{espace différentiable}\index{espace difféologique}\noindent Nous utilisons dans cette annexe une terminologie empruntée à la théorie des {\em espaces différentiables} ou {\em espaces difféologiques},dont on trouvera deux différentes variantes (équivalentes) dans \cite{Chen1} et \cite{Souriau3},et à la théorie des fibrées de cette catégorie d'espaces développée dans \cite{Iglesias2}.Nous en rappelons quelques éléments.Un paramétrage d'un ensemble $X$ est une application $P$ d'un ouvert $U$ d'un espace numérique quelconque $\RR^n$ dans $X$.Un {\em espace différentiable} est un ensemble $X$ pour lequel on a choisi un ensemble de paramétrages dits différentiables,qui sont appelés {\em plaques},et qui vérifient les propriétés suivantes:\begin{enumerate}  \item Les paramétrages constants sont des plaques;\index{plaques}  \item le plus petit prolongement commun d'une famille compatible de plaques est encore une plaque;  \item le composé d'une plaque par un paramétrage $\Cinf$ de sa source est encore une plaque.\end{enumerate}Les variétés sont évidemment des espaces différentiables.Mais les quotients de variétés sont aussi des espaces différentiables:une plaque du quotient est un paramétrage qui admet un relevé différentiable local au voisinage de tout point.C'est muni de cette structure quotient que le tore des périodes $T_\omega$ est considéré.Une application $F$ d'un espace différentiable $X$ dans un autre $Y$ est dite {\em différentiable} si le composé d'une plaque de $X$ par $F$ est une plaque de $Y$.Les difféomorphismes de $X$ à $Y$ sont évidemment les applications différentiables bijectives dont les inverses sont aussi différentiables.L'espace $\Cinf(X,Y)$ des applications différentiables de $X$ dans $Y$ est muni naturellement d'une structure différentiable appelée {\em structure fonctionnelle}.Les plaques de cette structure sont les paramétrages $r\mapsto f$ tels que l'application $(r,x)\mapsto f(x)$ soit différentiable.C'est de cette structure différentiable dont est muni l'espace $\Arc(X)$.On sait définir sur ces espaces la notion de fibrés différentiables et la théorie de l'homotopie qui l'accompagne \cite{Iglesias2}.La relation d'homotopie est définie à partir des familles à un paramètre d'arcs différentiables,indépendamment du choix d'une topologie {\em a priori\/}\footnote{Il existe toutefois,sur les espaces différentiables,une topologie remarquable (appelée {\em D-topologie}),la plus fine qui rende les plaques continues.}.\begin{itemize}  \item Une projection d'espaces différentiables $\pi : Y\to X$ est une {\em fibration} si,  pour toute plaque $P: U\to X$,  l'image réciproque $P^*(Y)\to U$ de la projection $P$ est localement triviale.\end{itemize}On montre que tout espace différentiable $X$ possède un revêtement universel,simplement connexe,unique à équivalence près,de groupe structural $\pi_1(X)$.Par exemple,la projection $\cl_\omega: \RR\to T_\omega$ réalise le revêtement universel de $T_\omega$.La notion de groupe différentiable est immédiate:un groupe différentiable est un groupe $G$ muni d'une structure différentiable compatible avec la multiplication et l'inversion.L'{\em algèbre de Lie} $\cG$ de $G$ est défini comme l'espace des homomorphismes différentiables de $\RR$ dans $G$:$\cG = \Hom^\infty(\RR,G)$.Dans le cas du tore $T_\omega$,les homomorphismes différentiables de $\RR$ dans $G$ se relèvent au revêtement universel $\cl_\omega : \RR\to T_\omega$ (théorème de monodromie des espaces différentiables \cite{Iglesias2}) en des homomorphismes de $\RR$ dans $\RR$ et donc $\cT_\omega \simeq \RR$.Une $p$-forme différentielle $\omega$ sur un espace différentiable $X$ est un procédé,qui à toute plaque $P$ de $X$ associe une $p$-forme notée $\omega(P)$ ou encore $P^*\omega$,définie sur le domaine $U$ de $P$,telle que pour tout paramétrage $\Cinf$,$F$ de $U$:$$  (P\circ F)^*\omega = F^*(P^*\omega).  $$La dérivée extérieure de $\omega$ est alors définie par$$  P^*[d\omega] = d[P^*\omega].  $$Cette définition donne lieu à un calcul différentiel extérieur sur $X$ qui coïncide avec le calcul extérieur ordinaire lorsque $X$ est une variété.L'image réciproque $f^*\omega$ d'une $p$-forme $\omega$ sur $X$ par une application différentiable $f:Y\to X$ est définie par:$P^*[f^*\omega] = [f\circ P]^*\omega$,où $P$ est une plaque de $Y$.\begin{proposition}  [\sc Quotient de forme]\label{propformquot}  Soit $X$ un espace différentiable et $\pi :X\to Y$ une surjection,  $Y$ étant muni de sa structure quotient.  Pour qu'une forme différentielle $\alpha$ définie sur $X$ passe au quotient,  \cad pour qu'il existe une forme $\beta$ sur $Y$ telle que $\alpha =\pi^*\beta$,  il faut et il suffit que pour tout couple de plaques $(P,P')$ de $X$ telles que $\pi \circ P=\pi \circ P'$ on ait:  $\alpha(P)=\alpha(P')$.\end{proposition}\begin{demonstration}  Par définition de la structure quotient,  toute plaque $Q$ de $Y$ s'écrit:  $Q= \sup \pi\circ P_i$,  où les $P_i$ sont des plaques de $X$ et $\sup$ désigne le plus petit prolongement commun.  La forme $\beta$ est définie par:  $\beta(Q) = \sup \alpha(P_i)$.  En effet,  soit $U_i$ le domaine de définition de $P_i$;  restreintes à $U_{ij}=U_i\cap U_j$,  les plaques $P_i$ et $P_j$ vérifient $\pi \circ P_i=\pi \circ P_j$,  et donc:  $\alpha(P_i\mid U_{ij})=\alpha(P_j\mid U_{ij})$.  Les formes $\alpha(P_i)$ sont compatibles,  et ont donc un plus petit prolongement commun $\sup \alpha(P_i)$.  Par un raisonnement analogue,  on montre que $\beta(Q)$ ne dépend pas du choix des relevés $P_i$ de $Q$.  Les autres vérifications sont immédiates.\end{demonstration}Il existe sur les espaces différentiables une formule de Cartan pour la dérivée de Lie des formes différentielles.Soit $h$ un groupe à un paramètre de difféomorphismes de $X$:$h\in \Hom^\infty(\RR,\Diff(X))$,la dérivée de Lie d'une $p$-forme $\omega$ par $h$ est définie par:\begin{equation}  \DLie_h\omega = {\partial \over \partial t} \Big\{h(t)^*\omega\Big\}_{t=0}.\end{equation}Le contracté $i_h(\omega)$ de la $p$-forme $\omega$ par le groupe à un paramètre $h$ est donné par la construction suivante.Soit $P:U \to X$ une plaque de $X$ et $h\cdot P$ la plaque définie sur $\RR\times U$ par:\begin{equation}  \label{defhP}  h\cdot P: (t,r) \to h(t)\circ P(r) = h(t)\Big(P(r)\Big).\end{equation}La $(p-1)$-forme $i_h(\omega),$ évaluée sur la plaque $P$,est donnée par la formule :\begin{equation}  \label{defcontraction}  P^*[i_h(\omega)] = i_{\partial /\partial t}[(h\cdot P)^*\omega]_{\{0\}\times U}\end{equation}Cette formule étend la construction ordinaire sur les variétés,et préserve la formule de Cartan\footnote{Nous verrons une autre démonstration de cette formule de Cartan dans l'annexe \ref{AnnStokes}.}.\begin{theoreme}  \index{formule de Cartan}  Soit $X$ un espace différentiable,  $\omega$ une $p$-forme sur $X$ et $h$ un groupe à un paramètre de difféomorphismes de $X$.  On a l'identité:  \begin{equation}    \label{formCartan}    \DLie_h\omega = d[i_h(\omega)] + i_h[d\omega].  \end{equation}\end{theoreme}\begin{demonstration}  Soit $P$ une plaque de $X$ définie sur un domaine $U$,  par définition:  $$    [\DLie_h\omega](P) = {\partial \over \partial t}\bigg\{[\eta(t)^*\tilde\omega](P)\bigg\}_{t=0} = {\partial \over \partial t}\bigg\{[h(t)\circ P]^*\omega\bigg\}_{t=0}.    $$  Soit $j : U\mapsto \{0\}\times U$ définie par $j(r)=(0,r)$,  on a:  $$    [i_h(\omega)](P)=i_{\partial/\partial t}[\omega(h\cdot P)]_{\{0\}\times U},    $$  où $h\cdot P$ est définie par (\ref{defhP}),  \cad $[i_h(\omega)](P)=j^*[i_{\partial /\partial t}(\omega(h\cdot P))]$.  On peut donc écrire:  \renewcommand{\arraystretch}{1.5}  $$    \begin{array}{r@{\hs}c@{\hs}l}    [d(i_h\omega )](P) + [i_h(d\omega)](P) & = & d\{j^*(i_{\partial /\partial t}[\omega(h\cdot P)])\} + j^*\{i_{\partial /\partial t}(d[\omega(h\cdot P)])\} \\    & = & j^*\{ d\circ i_{\partial /\partial t}[\omega(h\cdot P)] + i_{\partial /\partial t}\circ d[\omega(h\cdot P)]\} \\    & = & j^* \{ \DLie_{\partial /\partial t}[\omega(h\cdot P)]\} \\    & = & j^* \left\{ {\partial \over \partial t} \big[h(t)^*\omega(h\cdot P)\big]_{t=0} \right\} \\    & = & {\partial \over \partial t}\Big\{[h(t)\circ h\cdot P \circ j]^*\omega\Big\}_{t=0}.    \end{array}    $$  Or $h(t)\circ h\cdot P\circ j(r) = h(t)(P(r))$,  c'est-à-dire $h(t)\circ h\cdot P\circ j = h(t)\circ P$;  on en déduit donc:  $$    \begin{array}{r@{\hs}c@{\hs}l}    [d(i_h\omega )](P) + [i_h(d\omega)](P) & = & {\partial \over \partial t}\Big\{[h(t)\circ P]^*\omega\Big\}_{t=0} \\    & = & {\partial \over \partial t}\Big\{h(t)^*[\omega(P)]\Big\}_{t=0},    \end{array}    $$  \renewcommand{\arraystretch}{1}  \cad  $$    [d(i_h\omega )] + [i_h(d\omega)] = {\partial \over \partial t}\Big\{h(t)^*\omega\Big\}_{t=0},    $$  c'est ce qu'il fallait démontrer.\end{demonstration}\begin{remarque}  Dans le cas d'une 1-forme $\alpha$,  le contracté avec un homomorphisme $h$ est une fonction réelle.  Pour en donner une expression simple,  considérons l'{\em application orbite} $\hat x_h$:  $$    \begin{array}{rrccc}    \hat x_h & : & \RR & \longrightarrow & X        \\    &   & t   & \longmapsto     & h(t)(x)    \end{array}    $$  On vérifie alors que :  \begin{equation}    \label{remcontr1form}    i_h(\alpha)(x) = \hat x_h^*(\alpha)_{t=0}(1).  \end{equation}  L'application $\hat x_h$ étant définie de $\RR$ dans $X$,  l'image réciproque $\hat x_h^*(\alpha)$ est une 1-forme sur $\RR$;  calculée au point $t=0$ et appliquée au vecteur $1$,  elle donne la valeur de $i_h(\alpha)$ au point $x$.  La vérification de cette formule est une application directe de la formule (\ref{defhP}) à ce cas particulier.\end{remarque}%%====================================================================% MARK: - Appendix E: Opérateur de chaîne-homotopie%%====================================================================\chapter{Opérateur de chaîne-homotopie}\label{AnnOCH}\index{opérateur d'homotopie}\noindent Soit $X$ une variété différentiable connexe,on désigne par $\Arc(X)$\index{arcs} son espace des arcs,c'est-à-dire l'espace des applications différentiables de $\RR$ à valeurs dans $X$:\begin{equation}  \Arc(X) = \Cinf(\RR,X).\end{equation}Les applications {\em source} et {\em but}\index{application but}\index{application source},notées $\source$ et $\but$,associent respectivement à tout arc $\gamma$ les points $\gamma(0)$ et $\gamma(1)$.Le choix de $\RR$ plutôt que l'intervalle $[0,1]$ est technique,pour assurer la différentiabilité de la juxtaposition des arcs et l'action du groupe des translations.L'espace des arcs de $X$ est muni de sa structure fonctionnelle d'espace différentiable (voir annexe \ref{AnnED}).À toute $p$-forme $\omega$ de $X$,on associe la $p$-forme sur $\Arc(X)$ définie par l'intégration de $\omega$ le long des arcs,ce que nous noterons:\begin{equation}  \label{formomtild}  \tilde \omega_\gamma = \int_\gamma \omega.\end{equation}L'expression de $\tilde \omega$ dans toute plaque (paramétrage différentiable) $P$ de l'espace $\Arc(X)$ est donnée par:\begin{equation}  P^*\tilde \omega = \int_0^1P_t^*\omega \ dt,\end{equation}où $P_t$ est la plaque de $X$ définie par $P_t(r)=P(r)(t)$.L'application linéaire $\Phi : \omega \mapsto \tilde \omega$ ainsi définie,est un morphisme du complexe différentiel de De Rham de $X$ dans celui de $\Arc(X)$:\begin{equation}  \Phi : \Omega^*(X) \rightarrow \Omega^*(\Arc(X)) \quad \mbox{et} \quad d\circ \Phi = \Phi \circ d.\end{equation}Les translations de $\RR$ agissent comme un groupe à un paramètre $\eta$ de difféomorphismes de $\Arc(X)$,pour tout $\gamma \in \Arc(X)$ et tout $a\in \RR$:\begin{equation}  \eta(a): \gamma \mapsto \gamma \circ T_a \quad \mbox{où} \quad T_a : t\mapsto t+a.\end{equation}\begin{proposition}  La dérivée de Lie de la $p$-forme $\tilde\omega$ par $\eta$ vérifie:  \begin{equation}    \DLie_\eta \tilde\omega = \but^*\omega -\source^*\omega.  \end{equation}\end{proposition}\begin{demonstration}  En effet,  pour toute plaque $P$ de $\Arc(X)$:  $$    [\DLie_\eta\tilde\omega](P) = {\partial \over \partial t}\bigg\{[\eta(t)^*\tilde\omega](P)\bigg\}_{t=0} = {\partial \over \partial t}\bigg\{(\eta(t)^*\circ P)^*\tilde\omega\bigg\}_{t=0}.    $$  Mais $\eta(t)\circ P : r\mapsto P(r)\circ T_t$ et donc:  $$    (\eta(t)\circ P)^*\tilde \omega = \int_0^1(\eta(t)\circ P)_s^*\omega\ ds = \int_0^1[r\mapsto P(r)(t+s)]^*\omega\ ds.    $$  En posant $u=t+s$,  on obtient  $$    (\eta(t)\circ P)^*\tilde \omega = \int_t^{1+t}[r\mapsto P(r)(u)]^*\omega\ du,    $$  ce qui donne en dérivant:  $$    [\DLie_\eta\tilde\omega](P) = [r\mapsto P(r)(1)]^*\omega - [r\mapsto P(r)(0)]^*\omega =[\but^*\omega-\source^*\omega](P).    $$  C'est ce qu'il fallait démontrer.\end{demonstration}En appliquant alors la formule de Cartan (\ref{formCartan}),on a:$$  d[i_\eta\Phi(\omega)]+i_\eta\circ d[\Phi (\omega)] = \but^*\omega - \source^*\omega.  $$En posant alors:\begin{equation}  \label{defOH}  K = i_\eta \circ \Phi , \quad K:\Omega^*(X) \to \Omega^{*-1}(\Arc(X,\xo)),\end{equation}on obtient l'identité:\begin{equation}  \label{ophom}  K\circ d+d\circ K = \but^*-\source^*.\end{equation}\begin{definition}  Le morphisme $K$ ci-dessus est appelé {\em opérateur de chaîne-homotopie}.\end{definition}Lorsque $X$ est une variété,on peut donner une expression de l'opérateur $K$ en termes d'intégrales de chemins.Soit $P$ une plaque de $\Arc(X)$ définie sur un domaine $U$,soit $r\in U$ et $\delta r$ un vecteur tangent en $r$.Notons$$  \gamma =P(r) \quad \mbox{et} \quad \delta\gamma : t\mapsto D[\gamma(t)](r)(\delta r),  $$alors $K\omega$,calculé au point $\gamma$ et appliqué à $p-1$ {\em variations} $\gamma_i$ associées à des vecteurs $\delta r_i$ est donné par:\begin{equation}  \label{formuleK}  K\omega_\gamma(\delta \gamma_1,\ldots,\delta \gamma_{p-1}) =\int_0^1\omega_{\gamma(t)}(\dot\gamma(t),\delta \gamma_1,\ldots,\delta \gamma_{p-1})\ dt.\end{equation}%%====================================================================% MARK: - Appendix F: Variation d'intégrales multiples%%====================================================================\chapter{Variation d'intégrales multiples dans les espaces différentiables}\label{AnnStokes}\noindent Nous allons montrer,dans cette annexe,comment la variation d'une famille d'intégrales,définie sur un espace différentiable,permet de généraliser à ces espaces la formule de Stokes et donne une nouvelle méthode de démonstration de la formule de Cartan.Nous choisirons pour cela d'utiliser l'homologie cubique,plus pratique,même s'il n'est pas difficile de réécrire ce paragraphe en utilisant l'homologie simpliciale ordinaire,adaptée naturellement aux espaces différentiables.Soit $X$ un espace différentiable,nous appellerons {\em $p$-cube} de $X$ toute application différentiable $c$ définie sur un voisinage $U$ de $I^p$ à valeurs dans $X$,avec $I=$ (c'est en particulier une $p$-plaque de $X$).L'homologie cubique est définie de façon ordinaire,en considérant les différents groupes abéliens libres $C_p(X)$ engendrés par les $p$-cubes,$p\in \NN$.Les éléments de $C_p(X)$ sont les {\em $p$-chaînes cubiques} de $X$.Soit $\epsilon\in \{0,1\}$,on notera $j_i^\epsilon$ l'application de $\RR^{p-1}$ dans $\RR^p$ définie par:\begin{equation}  j_i^\epsilon : (t_1,\ldots,t_{p-1}) \to(t_1,\ldots,t_{i-1},\epsilon,t_{i+1},\ldots,t_{p-1}).\end{equation}À chaque indice $i=1,\ldots p$ correspond les deux faces du cube $c$:\begin{equation}  c_i^\epsilon = c\circ j_i^\epsilon , \quad i=0,1,\end{equation}ce qui permet de définir le bord $\partial c$ d'un $p$-cube $c$ par:\begin{equation}  \partial c = \sum_{i=1}^p (-1)^i (c_i^0 - c_i^1).\end{equation}L'opérateur $\partial$ est ensuite prolongé par linéarité sur $C_p(X)$ tout entier;il est à valeurs dans $C_{p-1}$ ($p\geq 1$).L'intégrale d'une $p$-forme $\omega\in \Omega^p(X)$ sur une $p$-chaîne $\sigma\in C_p(X)$ est obtenue en prolongeant par linéarité l'intégrale de $\omega$ sur les $p$-cubes:\begin{equation}  \sigma = \sum_{c\in C_p(X)} n_{c_p} c \quad \Rightarrow \quad \int_\sigma \omega = \sum_{c\in C_p(X)} n_{c_p} \int_c\omega,\end{equation}où les entiers $n_c$ sont tous nuls sauf un nombre fini d'entre eux,avec:\begin{equation}  \int_c \omega = \int_{I^p} c^*\omega.\end{equation}Considérons maintenant une famille différentiable de $p$-cubes $c_\alpha$,\cad une application différentiable définie sur $]-\epsilon,\epsilon[\times I^p$ à valeurs dans $X$.L'application $\alpha\mapsto \int_{c_\alpha}\omega_\alpha$ est alors différentiable,et nous noterons:\begin{equation}  \delta \int_c\omega = {\partial \over \partial \alpha} \left\{\int_{c_\alpha}\omega\right\}_{\alpha =0}\end{equation}la variation première de l'intégrale de $\omega$ sur $c$ associée à la famille $c_\alpha$.Soit $\bar c : ]-\epsilon,\epsilon[\times U \to X $ et $\bar\omega$ les $(p+1)$-plaques et $p$-formes définies respectivement par:\begin{equation}  \bar c = (\alpha,t) \mapsto c_\alpha(t) \quad \mbox{et} \quad \bar \omega =\omega(\bar c).\end{equation}Soit $t_0=\alpha$ et $e_0$ le vecteur de base $\partial/\partial \alpha =(1,0,\ldots,0)$ de $\RR\times \RR^p$,nous introduirons pour la commodité des expressions finales les quantités suivantes:\begin{equation}  \int_c d\omega(\delta c) = \int_{\{0\}\times I^p}[d\bar\omega](e_0)\end{equation}et\begin{equation}  \int_{\partial c}\omega (\delta c) = \sum_{\epsilon=0}^1 (-1)^{\epsilon}\sum_{i=0}^p\int_{I^{p-1}} {j_i^\epsilon}^*\bar \omega(e_0),\end{equation}où la notation $\bar \omega(e_0)$ désigne le contracté de $\bar\omega$ par le vecteur $e_0$ ({\em idem} pour $d\bar\omega$).Il serait possible de définir formellement $\delta c$,mais nous dirons simplement que cela représente la variation infinitésimale du cube $c$ associée à la famille $c_\alpha$.Le lecteur peut vérifier que,compte tenu de ces définitions,la démonstration de la proposition suivante se réduit à un calcul de géométrie différentielle ordinaire sur $\RR^p$.\begin{proposition}  Soit $X$ un espace différentiable,  $\omega$ une $p$-forme sur $X$ et $c$ un $p$-cube de $X$.  Soit $c_\alpha$ une famille différentiable de $p$-cubes telle que $c_0=c$.  La variation première de l'intégrale de $\omega$ sur $c$ associée à la famille $c_\alpha$ est donnée par la formule:  \begin{equation}    \label{varint1}    \delta \int_c\omega = \int_c d\omega(\delta c) + \int_{\partial c}\omega (\delta c).  \end{equation}\end{proposition}On obtient en particulier comme corollaire la formule de Stokes étendue aux espaces différentiables.\begin{corollaire}  Soit $\omega$ une $(p-1)$-forme définie sur un espace différentiable $X$,  et $\sigma$ une $p$-chaîne cubique de $X$,  on a:  \begin{equation}    \int_\sigma d\omega = \int_{\partial \sigma} \omega.  \end{equation}\end{corollaire}\begin{demonstration}  En effet,  il suffit de le démontrer pour un $p$-cube $c$.  On a $\delta \int_c d\omega = \int_{\partial c}d\omega (\delta c)$ et aussi $\delta \int_{\partial c}\omega = \int_{\partial c}d\omega (\delta c)$,  car $dd\omega =0$ et $\partial\partial c=0$.  On en déduit que $\delta[\int_c d\omega - \int_{\partial c}\omega]=0$,  et donc que $\int_c d\omega - \int_{\partial c}\omega$ est constante sur l'espace des $p$-cubes.  Comme l'espace des $p$-cubes est contractile,  il suffit d'évaluer cette différence sur le $p$-cube constant,  ce qui donne le résultat.\end{demonstration}On peut étendre évidemment cette proposition par linéarité à toute $p$-chaîne $\sigma$,limite de $p$-chaînes cubiques,pour laquelle l'intégrale converge.On peut l'étendre aussi sans difficulté à des domaines polyédraux en utilisant les $p$-chaînes simpliciales plutôt que cubiques.La formule (\ref{varint1}) ci-dessus peut se généraliser immédiatement au cas où la forme $\omega$ est aussi variable.Soit $\omega_\alpha$ une famille différentiable de $p$-formes telle que $\omega_0=\omega$.On dira que la famille de $p$-formes $\omega_\alpha$ est différentiable si pour toute plaque $P$ de $X$ l'application $(\alpha,r)\mapsto [\omega_\alpha(P)]_r$ est différentiable,autrement dit,si pour toute famille de $p$-vecteurs $(u_1,\ldots,u_p)$ dans $\RR^p$ l'application $(\alpha,r)\mapsto[\omega_\alpha(P)]_r(u_1,\ldots,u_p)$ est différentiable.En notant $\delta \omega$ la $p$-forme définie par:\begin{equation}  \delta \omega : P \mapsto {\partial \over \partial \alpha}\{\omega_\alpha (P)\}_{\alpha =0},\end{equation}on peut écrire:\begin{equation}  \label{varint}  \delta \int_c\omega = \int_c d\omega(\delta c) + \int_{\partial c}\omega (\delta c) + \int_c \delta \omega.\end{equation}La formule de Stokes permet de montrer en particulier,comme dans le cas des variétés,l'invariance par homotopie de la cohomologie de De Rham.Elle permet aussi,sous sa forme étendue (\ref{varint}),de donner une autre démonstration de la formule de Cartan (annexe \ref{AnnED}).\begin{demonstration}  {\sc(Formule de Cartan)}\index{formule de Cartan}  Soit $h$ un groupe à un paramètre de difféomorphismes de $X$,  $\omega$ une $p$-forme sur $X$ et $c$ un $p$-cube.  Notons $c_t=h(t)\circ c$ et $\omega_t = h(t)_*\omega =h(-t)^*\omega$.  En appliquant (\ref{varint}) à ces familles à un paramètre et en notant que $\int_{c_t}\omega_t= \int_c\omega$,  on obtient:  \begin{equation}    \int_c\DLie_h\omega = \int_c i_h[d\omega] + \int_c d[i_h\omega].  \end{equation}  Cette égalité étant vérifiée pour tout $p$-cube $c$ de $X$,  on en déduit la formule de Cartan:  $\DLie_h\omega = i_h[d\omega] + d[i_h\omega]$.\end{demonstration}